import random
from abc import ABC
from typing import List, Dict, Optional
import numpy as np
from ale.config import NLPTask
from ale.corpus.corpus import Corpus
from ale.registry.registerable_teacher import TeacherRegistry
from ale.teacher.base_teacher import BaseTeacher
from ale.teacher.exploitation.aggregation_methods import AggregationMethod
from ale.trainer.base_trainer import Predictor, PredictionResult
from ale.trainer.prediction_result import TokenConfidence


@TeacherRegistry.register("entropy-confidence")
class EntropyTeacher(BaseTeacher, ABC):
    """
    Entropy teacher: chooses documents where the entropy is highest

    Applied to ER task:
        - aggregation by average:
            - Burr Settles and Mark Craven. “An Analysis of Active Learning Strategies
            for Sequence Labeling Tasks”. In: Proceedings of the 2008 Conference on
            Empirical Methods in Natural Language Processing. 2008, pp. 1070–1079. 
            https://www.scopus.com/record/display.uri?eid=2-s2.0-80053375448&doi=10.3115%2f1613715.1613855&origin=inward&txGid=45dc7ea64707e8c3f02f956eb478b174
            - J. Yao et al. “Looking Back on the Past: Active Learning with Historical 
            Evaluation Results”. In: IEEE Transactions on Knowledge and Data Engineering
            (2020). DOI: 10.1109/TKDE.2020.3045816.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098783086&doi=10.1109%2fTKDE.2020.3045816&partnerID=40&md5=32acbf2905b4524153f017f60efe88da
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
            - Tang, S., Liu, H., Almatared, M., Abudayyeh, O., Lei, Z., Fong, A.: Towards 
            Automated Construction Quantity Take-Off: An Integrated Approach to Information 
            Extraction from Work Descriptions. Buildings 12(3) (2022). https://doi.org/10.3390/buildings12030354
            - Kholghi, M., Sitbon, L., Zuccon, G., Nguyen, A.: External knowledge and query 
            strategies in active learning: A study in clinical information extraction. In: 
            International Conference on Information and Knowledge Management, Proceedings. vol. 19-23-Oct-2015, pp. 143–152 (2015).
            https://doi.org/10.1145/2806416.2806550
            - Agrawal, A., Tripathi, S., Vardhan, M.: Active learning approach using a modified 
            least confidence sampling strategy for named entity recognition. Progress in Artificial 
            Intelligence 10(2), 113–128 (2021). https://doi.org/10.1007/s13748-021-00230-w
            - Shardlow, M., Ju, M., Li, M., O’Reilly, C., Iavarone, E., McNaught, J., Ananiadou, 
            S.: A Text Mining Pipeline Using Active and Deep Learning Aimed at Curating Information 
            in Computational Neuroscience. Neuroinformatics 17(3), 391–406 (2019). https://doi.org/10.1007/s12021-018-9404-y
            - Han, X., Kwoh, C., Kim, J.J.: Clustering based active learning for biomedical Named Entity
            Recognition. In: Proceedings of the International Joint Conference on Neural Networks. vol. 2016-October, pp. 1253–1260 (2016).
            https://doi.org/10.1109/IJCNN.2016.7727341
            - Chen, Y., Lasko, T.A., Mei, Q., Denny, J.C., Xu, H.: A study of active learning methods 
            for named entity recognition in clinical text. Journal of Biomedical Informatics 58, 11–18 
            (Dec 2015). https://doi.org/10.1016/j.jbi.2015.09.010
            - Tchoua, R., Ajith, A., Hong, Z., Ward, L., Chard, K., Audus, D., Patel, S., De Pablo, J.,
            Foster, I.: Active learning yields better training data for scientific named entity recognition. In: Proceedings - IEEE 15th
            - Claveau, V., Kijak, E.: Strategies to select examples for active learning with conditional 
            random fields. In: Lecture Notes in Computer Science (Including SubseriesLecture Notes in 
            Artificial Intelligence and Lecture Notes in Bioinformatics). vol. 10761 LNCS, pp. 30–43 (2018). https://doi.org/10.1007/978-3-319-77113-7
            - Nguyen, V., Lee, W., Ye, N., Chai, K., Chieu, H.: Active learning for probabilistic 
            hypotheses using the maximum Gibbs error criterion. In: Advances in Neural Information Processing Systems (2013)
        - aggregation by sum:
            - Burr Settles and Mark Craven. “An Analysis of Active Learning Strategies
            for Sequence Labeling Tasks”. In: Proceedings of the 2008 Conference on
            Empirical Methods in Natural Language Processing. 2008, pp. 1070–1079. 
            https://www.scopus.com/record/display.uri?eid=2-s2.0-80053375448&doi=10.3115%2f1613715.1613855&origin=inward&txGid=45dc7ea64707e8c3f02f956eb478b174
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
        - aggregation by maximum: 
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946

    """

    def __init__(
        self,
        corpus: Corpus,
        predictor: Predictor,
        seed: int,
        labels: List[any],
        nlp_task: NLPTask,
        aggregation_method: Optional[AggregationMethod]
    ):
        super().__init__(
            corpus=corpus,
            predictor=predictor,
            seed=seed,
            labels=labels,
            nlp_task=nlp_task,
            aggregation_method=aggregation_method
        )
        random.seed(self.seed)

    def propose(self, potential_ids: List[int], step_size: int, budget: int) -> List[int]:
        batch = random.sample(potential_ids, budget)
        idx2text = self.corpus.get_text_by_ids(batch)
        prediction_results: Dict[int, PredictionResult] = self.predictor.predict(
            idx2text)
        out_ids: List[int] = self.compute_function(
            prediction_results, step_size)

        return out_ids

    def compute_ner(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        """
        Entropy is calculated on token-level and aggregated on instance-level as configured.
        """
        scores = dict()
        for idx, prediction in predictions.items():
            token_confidences: List[TokenConfidence] = prediction.ner_confidences_token
            confidence_scores: List[float] = []
            for token in token_confidences:
                confidences: List[float] = [
                    label_confidence.confidence for label_confidence in token.label_confidence]
                entropy = -sum([pk * np.log(pk) for pk in confidences])
                confidence_scores.append(entropy)
            instance_margin = self.aggregate_function(confidence_scores)
            scores[idx] = instance_margin
        sorted_dict_by_score = sorted(
            scores.items(), key=lambda x: x[1], reverse=True)
        out_ids = [item[0] for item in sorted_dict_by_score[:step_size]]

        return out_ids

    def compute_cls(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        raise NotImplementedError(
            "Entropy teacher is not implemented for text classification.")
