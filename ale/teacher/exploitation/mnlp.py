import random
from abc import ABC
from typing import List, Dict, Optional
import numpy as np
from ale.config import NLPTask
from ale.corpus.corpus import Corpus
from ale.registry.registerable_teacher import TeacherRegistry
from ale.teacher.base_teacher import BaseTeacher
from ale.teacher.exploitation.aggregation_methods import AggregationMethod
from ale.trainer.base_trainer import Predictor, PredictionResult
from ale.trainer.prediction_result import TokenConfidence, LabelConfidence


@TeacherRegistry.register("mnlp")
class MNLPTeacher(BaseTeacher, ABC):
    """
    Maximum normalized log probability teacher: chooses documents where the normalized log probability is highest

        Applied to ER task:
        - aggregation by maximum: 
            - Li, W., Du, Y., Li, X., Chen, X., Xie, C., Li, H., Li, X.: UD BBC: Named entity 
            recognition in social network combined BERT-BiLSTM-CRF with active learning. 
            Engineering Applications of Artificial Intelligence (2022). https://doi.org/10.1016/j.engappai.2022.105460
            - Pradhan, A., Todi, K., Selvarasu, A., Sanyal, A.: Knowledge Graph Generation with 
            Deep Active Learning. In: Proceedings of the International Joint Conference
            on Neural Networks (2020). https://doi.org/10.1109/IJCNN48605.2020.9207515
            - Linh, L., Nguyen, M.T., Zuccon, G., Demartini, G.: Loss-based Active Learning for 
            Named Entity Recognition. In: Proceedings of the International Joint Conference on 
            Neural Networks. vol. 2021-July (2021). https://doi.org/10.1109/IJCNN52387.2021.9533675
            - Shen, Y., Yun, H., Lipton, Z., Kronrod, Y., Anandkumar, A.: Deep active learning
            for named entity recognition. In: Proceedings of the 2nd Workshop on Representation 
            Learning for NLP, Rep4NLP 2017 at the 55th Annual Meeting of the Association for 
            Computational Linguistics, ACL 2017. pp. 252–256 (2017)
            - Shen, Y., Yun, H., Lipton, Z.C., Kronrod, Y., Anandkumar, A.: Deep Active Learning for 
            Named Entity Recognition (Feb 2018)
            - Siddhant, A., Lipton, Z.C.: Deep Bayesian Active Learning for Natural Language Processing: 
            Results of a Large-Scale Empirical Study (Sep 2018)
            - Chang, H.S., Vembu, S., Mohan, S., Uppaal, R., McCallum, A.: Using error decay prediction 
            to overcome practical issues of deep active learning for named entity recognition. Machine 
            Learning 109(9-10), 1749–1778 (2020). https://doi.org/10.1007/s10994-020-05897-1
            - Zhou, S., Liang, S., Yang, Q., Jiang, W., He, Y., Li, Y.: Active Learning Based Labeling 
            Method for Fault Disposal Pre-plans. In: Advances and Trends in Artificial Intelligence. 
            Theory and Applications. pp. 377–382 (2023)
            - Lin, B., Lee, D.H., Xu, F., Lan, O., Ren, X.: AlpacaTag: An active learning-based crowd 
            annotation framework for sequence tagging. In: ACL 2019 - 57th Annual Meeting of the 
            Association for Computational Linguistics, Proceedings of System Demonstrations. pp. 58–63 (2019)
            - Van Nguyen, M., Ngo, N., Min, B., Nguyen, T.: FAMIE: A Fast Active Learning Framework for 
            Multilingual Information Extraction. In: NAACL 2022 - 2022 Conference of the North American 
            Chapter of the Association for Computational Linguistics: Human Language Technologies, 
            Proceedings of the Demonstrations Session. pp. 131–139 (2022)
            - Li, Y., Yue, T., Zhenxin, W.: IEKM-MD: An intelligent platform for information extraction
            and knowledge mining in multi-domains. In: CEUR Workshop Proceedings. vol. 2658, pp. 73–78 (2020)
            - Shelmanov, A., Puzyrev, D., Kupriyanova, L., Belyakov, D., Larionov, D., Khromov, N.,
            Kozlova, O., Artemova, E., Dylov, D.V., Panchenko, A.: Active Learning for Sequence Tagging 
            with Deep Pre-trained Models and Bayesian Uncertainty Estimates (Feb 2021)
            - Moniz, J., Patra, B., Gormley, M.: On Efficiently Acquiring Annotations for Multilingual 
            Models. In: Proceedings of the Annual Meeting of the Association for Computational 
            Linguistics. vol. 2, pp. 69–85 (2022)
    """

    def __init__(
        self,
        corpus: Corpus,
        predictor: Predictor,
        seed: int,
        labels: List[any],
        nlp_task: NLPTask,
        aggregation_method: Optional[AggregationMethod]
    ):
        super().__init__(
            corpus=corpus,
            predictor=predictor,
            seed=seed,
            labels=labels,
            nlp_task=nlp_task,
            aggregation_method=aggregation_method
        )
        random.seed(self.seed)

    def propose(self, potential_ids: List[int], step_size: int, budget: int) -> List[int]:        
        search_for_least_confidence = random.sample(potential_ids, budget)
        idx2text = self.corpus.get_text_by_ids(search_for_least_confidence)
        prediction_results: Dict[int, PredictionResult] = self.predictor.predict(idx2text)
        out_ids: List[int] = self.compute_function(prediction_results, step_size)

        return out_ids
    
    def compute_ner(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        """
        MNLP is calculated on token-level and aggregated on instance-level as configured.
        """
        scores = dict()
        for idx,prediction in predictions.items():
            token_confidences: List[TokenConfidence] = prediction.ner_confidences_token
            confidence_scores: List[float] = []
            for token in token_confidences:
                confidences: List[float] = [label_confidence.confidence for label_confidence in token.label_confidence]
                log_probabilities = [np.log(pk) for pk in confidences]
                confidence_scores.append(max(log_probabilities))
            instance_margin = self.aggregate_function(confidence_scores)
            scores[idx] = instance_margin
        sorted_dict_by_score = sorted(scores.items(), key=lambda x:x[1], reverse=True)
        out_ids = [item[0] for item in sorted_dict_by_score[:step_size]]
        
        return out_ids

    def compute_cls(self, predictions: Dict[int,PredictionResult], step_size: int) -> List[int]:
        raise NotImplementedError("MNLP teacher is not implemented for text classification yet.")

