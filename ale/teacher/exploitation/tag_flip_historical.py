import random
from abc import ABC
from typing import List, Dict, Optional, Any

from mlflow.entities import Run

from ale.config import NLPTask
from ale.corpus.corpus import Corpus
from ale.registry.registerable_teacher import TeacherRegistry
from ale.teacher.base_teacher import BaseTeacher
from ale.teacher.exploitation.aggregation_methods import AggregationMethod
from ale.trainer.prediction_result import PredictionResult
from ale.trainer.predictor import Predictor


class HistoricalDoc:
    def __init__(self, idx: int, prediction_result: PredictionResult) -> None:
        """
        Helper class used to save predictions of last cycle for one document.
        """
        self.idx: int = idx
        self.predictions: List[List[str]] = []
        self.predictions.append([token_confidence.get_predicted_label()
                                 for token_confidence in prediction_result.ner_confidences_token])

    def add_new_prediction(self, predictions: PredictionResult):
        """
        Add the new predicted labels to the list of labels

        Args:
            - predictions (PredictionResult): prediction result of current cycle


        """
        new_predictions = [token_confidence.get_predicted_label()
                      for token_confidence in predictions.ner_confidences_token]

        if len(new_predictions) != len(self.predictions[0]):
            raise AssertionError("Length of label sequence must always be of equal length for one document.")

        self.predictions.append(new_predictions)

    def compute_diffs(self) -> List[int]:
        """
        Compares old and new labels and returns a list indicating which label predictions have changed (per token).

        Returns:
            - difference (List[int]): a list which indicates how often the token-predictions has changed over time
        """
        if len(self.predictions) == 1:
            return [0 for i in range(self.get_doc_length())]

        # Formula 9 of paper
        difference: List[int] = []
        for t in range(self.get_doc_length()):
            number_diffs_for_token_over_time = 0
            for e in range(1, len(self.predictions)):
                if self.predictions[e][t] != self.predictions[e - 1][t]:
                    number_diffs_for_token_over_time += 1
            difference.append(number_diffs_for_token_over_time)

        return difference

    def get_doc_length(self) -> int:
        """
        Returns the doc length in number of tokens
        """
        return len(self.predictions[0])


class HistoricalSequence:
    def __init__(self, docs_with_predictions: Dict[int, PredictionResult]) -> None:
        """
        Helper class used to wrap predictions of last cycle.

        Args:
            - docs_with_predictions (Dict[int,PredictionResult]): Dict containing the ids of the documents
                                                                  and their prediction result
        """
        self.docs: Dict[int, HistoricalDoc] = {}
        for doc_idx, predictions in docs_with_predictions.items():
            historical_doc: HistoricalDoc = HistoricalDoc(doc_idx, predictions)
            self.docs[doc_idx] = historical_doc

    def update_and_compare_historical_sequences(self,
                                                docs_with_predictions: Dict[int, PredictionResult]) -> Dict[
        int, List[int]]:
        differences: Dict[int, List[int]] = {}
        for doc_idx, prediction in docs_with_predictions.items():
            historical_doc: HistoricalDoc = self.update_historical_sequence(doc_idx, prediction)
            differences[doc_idx] = historical_doc.compute_diffs()
        return differences

    def update_historical_sequence(self, doc_idx: int, prediction: PredictionResult) -> HistoricalDoc:
        historical_doc: Optional[HistoricalDoc] = self.get_historical_doc_by_id(doc_idx)
        if historical_doc:  # history exists for document
            historical_doc.add_new_prediction(prediction)
        else:
            historical_doc = HistoricalDoc(doc_idx, prediction)
            self.docs[doc_idx] = historical_doc
        return historical_doc

    def get_historical_doc_by_id(self, idx: int) -> Optional[HistoricalDoc]:
        if idx not in self.docs:
            return None

        return self.docs[idx]


@TeacherRegistry.register("tag-flip-historical")
class TagFlipTeacher(BaseTeacher, ABC):
    """
    Tag Flip teacher using historical sequences: chooses documents with most tag flips in comparison to last cycle
    Should be used for initial train as well to save predictions already for initial train.

        Applied to ER task:
        - aggregation by sum: 
            - Zheng, G., Mukherjee, S., Dong, X., Li, F.: OpenTag: Open attribute value extraction from product
            profiles. In: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and 
            Data Mining. pp. 1049â€“1058 (2018). https://doi.org/10.1145/3219819.3219839
    """

    def __init__(
            self,
            corpus: Corpus,
            predictor: Predictor,
            seed: int,
            labels: List[any],
            nlp_task: NLPTask,
            aggregation_method: Optional[AggregationMethod]
    ):
        super().__init__(
            corpus=corpus,
            predictor=predictor,
            seed=seed,
            labels=labels,
            nlp_task=nlp_task,
            aggregation_method=aggregation_method
        )
        random.seed(self.seed)
        self.historical_sequence = None

    def propose(self, potential_ids: List[int], step_size: int, budget: int) -> List[int]:
        if budget < len(potential_ids):
            batch: List[int] = random.sample(potential_ids, budget)
        else:
            batch: List[int] = potential_ids
        idx2text = self.corpus.get_text_by_ids(batch)
        prediction_results: Dict[int, PredictionResult] = self.predictor.predict(idx2text)
        out_ids: List[int] = self.compute_function(prediction_results, step_size)

        return out_ids

    def compute_ner(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        """
        Tag Flip is calculated on token-level and aggregated on instance-level as configured.
        """
        scores: Dict[int, int] = dict()  # doc idx to number of changes

        if self.is_initial_train():
            next_batch = random.sample(list(predictions.keys()), step_size)
            self.historical_sequence: HistoricalSequence = HistoricalSequence(predictions)
            return next_batch

        differences: Dict[int, List[int]] = self.historical_sequence.update_and_compare_historical_sequences(
            predictions)
        for idx, diff in differences.items():
            instance_score = self.aggregate_function(diff)
            scores[idx] = instance_score

        sorted_dict_by_score = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        out_ids = [item[0] for item in sorted_dict_by_score[:step_size]]

        return out_ids

    def compute_cls(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        raise NotImplementedError(
            "Tag Flip teacher is not implemented for text classification yet.")

    def is_initial_train(self) -> bool:
        return not self.historical_sequence

    def store_state(self, run: Run):
        self.store_state_objects(run, {
            "historical_sequence.pkl": self.historical_sequence,
        })

    def restore_from_artifacts(self, run: Run):
        state_objects: Dict[str, Any] = self.restore_state_objects(run, ["historical_sequence.pkl"])
        self.historical_sequence = state_objects["historical_sequence.pkl"]
