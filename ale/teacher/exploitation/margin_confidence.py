import random
from abc import ABC
from typing import List, Dict, Optional
import numpy as np
from ale.config import NLPTask
from ale.corpus.corpus import Corpus
from ale.registry.registerable_teacher import TeacherRegistry
from ale.teacher.base_teacher import BaseTeacher
from ale.teacher.exploitation.aggregation_methods import AggregationMethod
from ale.trainer.predictor import Predictor
from ale.trainer.prediction_result import TokenConfidence, LabelConfidence, PredictionResult


@TeacherRegistry.register("margin-confidence")
class MarginTeacher(BaseTeacher, ABC):
    """
    Margin teacher: chooses documents where the margin between the two most probable labels is lowest

        Applied to ER task:
        - aggregation by average:
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
            - Mejer, A., Crammer, K.: Confidence in structured-prediction using Confidence-Weighted 
            models. In: EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, 
            Proceedings of the Conference. pp. 971–981 (2010)
            - Tomanek, K., Laws, F., Hahn, U., Sch ̈utze, H.: On proper unit selection in active 
            learning: Co-selection effects for named entity recognition. In: Proceedings of the 
            NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing. pp. 9–17. 
            HLT ’09, Association for Computational Linguistics, USA (2009)
            - Laws, F., Scheible, C., Sch ̈utze, H.: Active learning with amazon mechanical turk.
            In: EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, 
            Proceedings of the Conference. pp. 1546–1556 (2011)
        - aggregation by minimum: 
            - Burr Settles and Mark Craven. “An Analysis of Active Learning Strategies
            for Sequence Labeling Tasks”. In: Proceedings of the 2008 Conference on
            Empirical Methods in Natural Language Processing. 2008, pp. 1070–1079. 
            https://www.scopus.com/record/display.uri?eid=2-s2.0-80053375448&doi=10.3115%2f1613715.1613855&origin=inward&txGid=45dc7ea64707e8c3f02f956eb478b174
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
            - Agrawal, A., Tripathi, S., Vardhan, M.: Active learning approach using a modified 
            least confidence sampling strategy for named entity recognition. Progress in Artificial 
            Intelligence 10(2), 113–128 (2021). https://doi.org/10.1007/s13748-021-00230-w
            - Skeppstedt, M., Rzepka, R., Araki, K., Kerren, A.: Visualising and evaluating the 
            effects of combining active learning with word embedding features. In: Proceedings
            of the 15th Conference on Natural Language Processing, KONVENS 2019. pp. 91–100 (2020)
            - Verma, M., Sikdar, U., Saha, S., Ekbal, A.: Ensemble based active annotation for 
            biomedical named entity recognition. In: Proceedings of the 2013 International Conference 
            on Advances in Computing, Communications and Informatics, ICACCI 2013. pp. 973–978 (2013).
            https://doi.org/10.1109/ICACCI.2013.6637308
            - Saha, S., Ekbal, A., Verma, M., Sikdar, U., Poesio, M.: Active learning technique 
            for biomedical named entity extraction. In: ACM International Conference
            Proceeding Series. pp. 835–841 (2012). https://doi.org/10.1145/2345396.2345532
            - Miller, S., Guinness, J., Zamanian, A.: Name tagging with word clusters and 
            discriminative training. In: HLT-NAACL 2004 - Human Language Technology Conference 
            of the North American Chapter of the Association for Computational Linguistics, 
            Proceedings of the Main Conference. pp. 337–342 (2004)
        - aggregation by sum: 
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
        - aggregation by maximum: 
            - Esuli, A., Marcheggiani, D., Sebastiani, F.: Sentence-based active learning strategies 
            for information extraction. In: CEUR Workshop Proceedings. vol. 560, pp. 41–45 (2010)
            - A.O.B. Şapci et al. “Focusing on Potential Named Entities during Active Label 
            Acquisition”. In: Natural Language Engineering (2023). DOI: 10.1017/S1351324923000165.
            https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162145912&doi=10.1017%2fS1351324923000165&partnerID=40&md5=340e4f2adb01a08f0cf0abd952a63946
            - Liu, M., Tu, Z., Zhang, T., Su, T., Xu, X., Wang, Z.: LTP: A New Active Learning Strategy 
            for CRF-Based Named Entity Recognition. Neural Processing Letters 54(3), 2433–2454 (2022). https://doi.org/10.1007/s11063-021-10737-x
            - Li, W., Du, Y., Li, X., Chen, X., Xie, C., Li, H., Li, X.: UD BBC: Named entity 
            recognition in social network combined BERT-BiLSTM-CRF with active learning. 
            Engineering Applications of Artificial Intelligence (2022). https://doi.org/10.1016/j.engappai.2022.105460
        - aggregation by standard deviation:
            -  Claveau, V., Kijak, E.: Strategies to select examples for active learning with conditional 
            random fields. In: Lecture Notes in Computer Science (Including SubseriesLecture Notes in 
            Artificial Intelligence and Lecture Notes in Bioinformatics). vol. 10761 LNCS, pp. 30–43 (2018). https://doi.org/10.1007/978-3-319-77113-7    

    """

    def __init__(
        self,
        corpus: Corpus,
        predictor: Predictor,
        seed: int,
        labels: List[any],
        nlp_task: NLPTask,
        aggregation_method: Optional[AggregationMethod]
    ):
        super().__init__(
            corpus=corpus,
            predictor=predictor,
            seed=seed,
            labels=labels,
            nlp_task=nlp_task,
            aggregation_method=aggregation_method
        )
        random.seed(self.seed)

    def propose(self, potential_ids: List[int], step_size: int, budget: int) -> List[int]:
        search_for_least_confidence = random.sample(potential_ids, budget)
        idx2text = self.corpus.get_text_by_ids(search_for_least_confidence)
        prediction_results: Dict[int, PredictionResult] = self.predictor.predict(
            idx2text)
        out_ids: List[int] = self.compute_function(
            prediction_results, step_size)

        return out_ids

    def compute_ner(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        """
        Margin is calculated on token-level and aggregated on instance-level as configured.
        """
        scores = dict()
        for idx, prediction in predictions.items():
            token_confidences: List[TokenConfidence] = prediction.ner_confidences_token
            margins: List[float] = []
            for token in token_confidences:
                confidence_scores: List[LabelConfidence] = token.get_highest_k(
                    2)
                margin = abs(
                    confidence_scores[0].confidence - confidence_scores[1].confidence)
                margins.append(margin)
            instance_margin = self.aggregate_function(margins)
            scores[idx] = instance_margin
        sorted_dict_by_score = sorted(scores.items(), key=lambda x: x[1])
        out_ids = [item[0] for item in sorted_dict_by_score[:step_size]]

        return out_ids

    def compute_cls(self, predictions: Dict[int, PredictionResult], step_size: int) -> List[int]:
        scores = dict()
        for idx, prediction in predictions.items():
            confidences = np.array(
                list(prediction.classification_confidences.values()))
            sorted_index = np.argsort(confidences)[-2:]
            diff = abs(confidences[sorted_index[0]] -
                       confidences[sorted_index[1]])
            scores[idx] = diff

        sorted_dict_by_score = sorted(scores.items(), key=lambda x: x[1])
        out_ids = [item[0] for item in sorted_dict_by_score[:step_size]]

        return out_ids
